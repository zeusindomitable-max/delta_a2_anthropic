@article{anthropic2023constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Ben and Chen, Anna and Mirhoseini, Azalia and McKinnon, Chris and Chen, Andy and Olsson, Catherine and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2023}
}

@article{ganguli2022redwood,
  title={Redwood Research Alignment Experiments: Towards Mechanistic Interpretability},
  author={Ganguli, Deep and Joseph, Nicholas and Schiefer, Nicholas and Olsson, Catherine and Nanda, Neel and Steinhardt, Jacob},
  journal={Technical Report},
  year={2022}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  year={2019}
}

@article{olsson2022mechanistic,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Nanda, Neel and Joseph, Nicholas and DasSarma, Jared and Elhage, Nelson and Henighan, Tom and Mann, Ben and Askell, Amanda and Jones, Ben and Bowman, Sam and others},
  journal={Transformer Circuits Thread},
  year={2022}
}

@article{bills2023scaling,
  title={Scaling Monosemanticity: Extracting Interpretable Features from Claude},
  author={Bills, Shane and Joseph, Nicholas and Casper, Stephen and Grosse, Roger and Bowman, Sam and others},
  journal={Anthropic Research Paper},
  year={2023}
}

@article{steinhardt2023alignment,
  title={AI Alignment: Toward a Research Program},
  author={Steinhardt, Jacob},
  journal={Alignment Forum Essays},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeff and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{bowman2023interpretability,
  title={Anthropic’s Experiments in Mechanistic Interpretability},
  author={Bowman, Sam and Bills, Shane and Casper, Stephen and Joseph, Nicholas and others},
  journal={Anthropic Interpretability Blog},
  year={2023}
}

@misc{zeus2025deltaa2,
  title={Δa₂ Alignment Toolkit: Empirical Correlation between Curvature Stability and Preference Alignment},
  author={ZeusIndomitable-Max},
  year={2025},
  note={GitHub repository: \url{https://github.com/zeusindomitable-max/delta-a2-alignment-toolkit}}
}
